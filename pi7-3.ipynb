{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sqlite3 as lite\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVR, LinearSVC\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of rows in races_df:  225918\n",
      "Amount of rows in riders_df:  1042\n",
      "Unique values for 'months':  [ 1  3  4  5  6  7  8  9 10] \n",
      "\n",
      "Unique values for 'years':  [2012 2014 2015 2017 2018 2020 2021]\n",
      "  Timelag  Timelag_seconds\n",
      "0   +0:00              NaN\n",
      "1   +0:04              4.0\n",
      "2   +0:06              6.0\n",
      "3   +0:10             10.0\n",
      "4   +0:10             10.0\n",
      "5   +0:10             10.0\n",
      "6   +0:10             10.0\n",
      "7   +0:10             10.0\n",
      "8   +0:10             10.0\n",
      "9   +0:10             10.0\n"
     ]
    }
   ],
   "source": [
    "conn = lite.connect('cycling_big.db')\n",
    "\n",
    "riders_df = pd.read_sql_query('SELECT * FROM riders;', conn)\n",
    "races_df = pd.read_sql_query('SELECT * FROM race_results', conn)\n",
    "\n",
    "print(\"Amount of rows in races_df: \", races_df[races_df.columns[0]].count())\n",
    "print(\"Amount of rows in riders_df: \", riders_df[riders_df.columns[0]].count())\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\"\"\"\n",
    "Convert the 'Date' column to DateTime format\n",
    "Regarding races_df dataframe!\n",
    "\"\"\"\n",
    "\n",
    "races_df['Date'] = pd.to_datetime(races_df['Date'], errors='coerce', format='%d %B %Y')\n",
    "\n",
    "# Remove rows where 'Date' is NaT\n",
    "races_df = races_df.dropna(subset=['Date'])\n",
    "\n",
    "# Normalize 'Date' to strip out time if it's present (this keeps just the date part)\n",
    "races_df['Date'] = races_df['Date'].dt.normalize()\n",
    "\n",
    "# Extract the month and year from the column and put them in their own columns\n",
    "races_df['Month'] = races_df['Date'].dt.month\n",
    "races_df['Year'] = races_df['Date'].dt.year\n",
    "\n",
    "print(\"Unique values for 'months': \", races_df['Month'].unique(), \"\\n\")\n",
    "print(\"Unique values for 'years': \",races_df['Year'].unique())\n",
    "\n",
    "\"\"\"\n",
    "Converting the timetable to total seconds\n",
    "\"\"\"\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    # Remove commas and any spaces\n",
    "    time_str = time_str.replace(',', '').strip()\n",
    "\n",
    "    # Check the consistency of the time format using regular expressions\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2}):(\\d{2})', time_str)\n",
    "    if match:\n",
    "        hours, minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "        if total_seconds == 0:\n",
    "            return np.nan\n",
    "        return total_seconds\n",
    "\n",
    "    # Do the same as the loop above, but now for MM:SS format.\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2})', time_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        if total_seconds == 0:\n",
    "            return np.nan  \n",
    "        return total_seconds\n",
    "\n",
    "    # Do the same but for 0:00, 0:01, etc.\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{1,2})', time_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        if total_seconds == 0:\n",
    "            return np.nan \n",
    "        return total_seconds\n",
    "    \n",
    "    # If format doesn't match, return NaN\n",
    "    return np.nan\n",
    "\n",
    "# Apply the conversion function to the 'Time' column\n",
    "races_df['Time_seconds'] = races_df['Time'].apply(time_to_seconds)\n",
    "\n",
    "# print(races_df[['Time', 'Time_seconds']].head(10))\n",
    "# races_df.info()\n",
    "\n",
    "\"\"\"\n",
    "Converting timelag to total seconds\n",
    "\"\"\"\n",
    "\n",
    "def timelag_to_seconds(timelag_str):\n",
    "    # Check if the timelag_str contains missing values\n",
    "    if pd.isna(timelag_str):\n",
    "        return np.nan\n",
    "\n",
    "    timelag_str = timelag_str.lstrip('+').strip()\n",
    "\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2}):(\\d{2})', timelag_str)\n",
    "    if match:\n",
    "        hours, minutes, seconds = map(int, match.groups())\n",
    "        return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2})', timelag_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        return minutes * 60 + seconds\n",
    "    \n",
    "    match = re.match(r'(\\d+):(\\d+)', timelag_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        return minutes * 60 + seconds\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "races_df['Timelag_seconds'] = races_df['Timelag'].apply(timelag_to_seconds)\n",
    "\n",
    "races_df['Timelag_seconds'] = races_df['Timelag_seconds'].replace(0.0, np.nan)\n",
    "\n",
    "print(races_df[['Timelag', 'Timelag_seconds']].head(10))\n",
    "\n",
    "\"\"\"\n",
    "Converting distance into single numerical value\n",
    "This means stripping 'km' from string and converting the remaining values into float64\n",
    "\"\"\"\n",
    "\n",
    "races_df['Length'] = races_df['Length'].str.replace(' km', '', regex=False)\n",
    "\n",
    "# Convert to numeric and replace 0 with NaN\n",
    "races_df['Length'] = pd.to_numeric(races_df['Length'], errors='coerce')  # Convert to numeric and handle errors\n",
    "\n",
    "# Replace 0 values with NaN\n",
    "races_df['Length'] = races_df['Length'].replace(0.0, np.nan)\n",
    "\n",
    "# print(races_df['Length'])\n",
    "\n",
    "\"\"\"\n",
    "Splitting values from 'rdr' and putting the split values into separate columns\n",
    "\"\"\"\n",
    "\n",
    "# Function to convert the 'rdr' string to separate ranking columns\n",
    "def extract_rankings(rdr_str):\n",
    "    try:\n",
    "        # Converting str to dict\n",
    "        rankings = ast.literal_eval(rdr_str)\n",
    "        \n",
    "        # Extracting ranks, while also handling missing keys\n",
    "        pcs_rnk = rankings.get('PCS Ranking', np.nan)\n",
    "        uci_rnk = rankings.get('UCI World Ranking', np.nan)\n",
    "        alltime_rnk = rankings.get('Specials | All Time Ranking', np.nan)\n",
    "        \n",
    "        return pd.Series([pcs_rnk, uci_rnk, alltime_rnk])\n",
    "    except:\n",
    "        return pd.Series([np.nan, np.nan, np.nan])\n",
    "\n",
    "# Apply the function to the 'rdr' column\n",
    "riders_df[['PCS_Rnk', 'UCI_Rnk', 'AllTime_Rnk']] = riders_df['rdr'].apply(extract_rankings)\n",
    "\n",
    "# Entries should be turned into numeric values, where errors get turned into NaN\n",
    "riders_df['PCS_Rnk'] = pd.to_numeric(riders_df['PCS_Rnk'], errors='coerce')\n",
    "riders_df['UCI_Rnk'] = pd.to_numeric(riders_df['UCI_Rnk'], errors='coerce')\n",
    "riders_df['AllTime_Rnk'] = pd.to_numeric(riders_df['AllTime_Rnk'], errors='coerce')\n",
    "\n",
    "# print(riders_df[['fullname', 'PCS_Rnk', 'UCI_Rnk', 'AllTime_Rnk']])\n",
    "\n",
    "\"\"\"\n",
    "Convert stage types to binary with label encoding\n",
    "\"\"\"\n",
    "\n",
    "races_df['Stage_Type_bin'] = races_df['Stage_Type'].map({'RR': 0, 'ITT': 1})\n",
    "\n",
    "\"\"\"\n",
    "Splitting values from 'pps' and putting those values into separate columns.\n",
    "\"\"\"\n",
    "\n",
    "def extract_points(pps_str):\n",
    "    try:\n",
    "        points = ast.literal_eval(pps_str)\n",
    "        \n",
    "        day_pnt = np.nan if points.get('One day races', '0') == '0' else points.get('One day races', np.nan)\n",
    "        gc_pnt = np.nan if points.get('GC', '0') == '0' else points.get('GC', np.nan)\n",
    "        tt_pnt = np.nan if points.get('Time trial', '0') == '0' else points.get('Time trial', np.nan)\n",
    "        sprint_pnt = np.nan if points.get('Sprint', '0') == '0' else points.get('Sprint', np.nan)\n",
    "        climb_pnt = np.nan if points.get('Climber', '0') == '0' else points.get('Climber', np.nan)\n",
    "        \n",
    "        return pd.Series([day_pnt, gc_pnt, tt_pnt, sprint_pnt, climb_pnt])\n",
    "    except:\n",
    "        return pd.Series([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "riders_df[['Day_Pnt', 'GC_Pnt', 'TT_Pnt', 'Sprint_Pnt', 'Climb_Pnt']] = riders_df['pps'].apply(extract_points)\n",
    "\n",
    "riders_df['Day_Pnt'] = pd.to_numeric(riders_df['Day_Pnt'], errors='coerce')\n",
    "riders_df['GC_Pnt'] = pd.to_numeric(riders_df['GC_Pnt'], errors='coerce')\n",
    "riders_df['TT_Pnt'] = pd.to_numeric(riders_df['TT_Pnt'], errors='coerce')\n",
    "riders_df['Sprint_Pnt'] = pd.to_numeric(riders_df['Sprint_Pnt'], errors='coerce')\n",
    "riders_df['Climb_Pnt'] = pd.to_numeric(riders_df['Climb_Pnt'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          BARDET Romain\n",
      "1        DUMOULIN Samuel\n",
      "2          GALLOPIN Tony\n",
      "3          NAESEN Oliver\n",
      "4          FRANK Mathias\n",
      "              ...       \n",
      "1037      TRONDSEN Trond\n",
      "1038    VAN MELSEN Kévin\n",
      "1039     BEULLENS Cédric\n",
      "1040    DE WINTER Ludwig\n",
      "1041      DELACROIX Théo\n",
      "Name: fullname, Length: 1042, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(riders_df['fullname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          BARDET_Romain\n",
      "1        DUMOULIN_Samuel\n",
      "2          GALLOPIN_Tony\n",
      "3          NAESEN_Oliver\n",
      "4          FRANK_Mathias\n",
      "              ...       \n",
      "1037      TRONDSEN_Trond\n",
      "1038    VAN_MELSEN_Kévin\n",
      "1039     BEULLENS_Cédric\n",
      "1040    DE_WINTER_Ludwig\n",
      "1041      DELACROIX_Théo\n",
      "Name: fullname, Length: 1042, dtype: object\n"
     ]
    }
   ],
   "source": [
    "riders_df['fullname'] = riders_df['fullname'].str.replace(' ', '_') \n",
    "print(riders_df['fullname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of rows in races_df post cleanup:  80174\n",
      "Amount of rows in riders_df post cleanup:  1042\n",
      "Amount of rows in df:  80174\n",
      "Rnk                    0\n",
      "GC                 10059\n",
      "BiB                    0\n",
      "Rider                  0\n",
      "Age                    0\n",
      "UCI                77527\n",
      "Pnt                69767\n",
      "Length              6495\n",
      "Month                  0\n",
      "Year                   0\n",
      "Time_seconds       18435\n",
      "Timelag_seconds    11212\n",
      "Stage_Type_bin     72484\n",
      "fullname           34208\n",
      "team               34208\n",
      "country            34208\n",
      "height             34260\n",
      "weight             34266\n",
      "PCS_Rnk            45842\n",
      "UCI_Rnk            46804\n",
      "AllTime_Rnk        53300\n",
      "Day_Pnt            34216\n",
      "GC_Pnt             34208\n",
      "TT_Pnt             34631\n",
      "Sprint_Pnt         34342\n",
      "Climb_Pnt          34365\n",
      "dtype: int64\n",
      "Amount of rows in df after cleanup:  80174\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Join the rider and race tables together, using the rider_id as an index\n",
    "\"\"\"\n",
    "print(\"Amount of rows in races_df post cleanup: \", races_df[races_df.columns[0]].count())\n",
    "print(\"Amount of rows in riders_df post cleanup: \", riders_df[riders_df.columns[0]].count())\n",
    "\n",
    "df = races_df.set_index('rider_id').join(riders_df.set_index('rider_id'), how = 'left')\n",
    "\n",
    "print(\"Amount of rows in df: \", df[df.columns[0]].count())\n",
    "\n",
    "\"\"\"\n",
    "Dropping columns that are not needed for analysis\n",
    "\"\"\"\n",
    "\n",
    "# Note: fix the long list\n",
    "df.drop(['Time', 'Timelag', 'rdr', 'pps', 'birthdate', 'rider_url', 'Race_url', 'Stage_url', 'Circuit', 'Race_Name', 'Stage_Name', 'Start', 'Finish', 'Category', 'Stage_Type'], axis=1, inplace=True)\n",
    "\n",
    "# Additional drops:\n",
    "df = df.drop(columns=['id', 'Team', 'Date', 'Race_ID', 'Stage_Number', 'Team'])\n",
    "# Replace all remaining zero values with NaN\n",
    "df = df.replace(0, np.nan)\n",
    "\n",
    "print(df.isna().sum())\n",
    "print(\"Amount of rows in df after cleanup: \", df[df.columns[0]].count())\n",
    "print(\"\\ndone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Rnk' or final rank will be used as the target value. This column still contains alot of numerical values for 'DNF', meaning this will need to get cleaned up. Removing non-numericals, like DNF, DNS and OTL, does not impact the amount of available data by a large amount, so we will result in dropping these entries. Since these entries will most likely not contain any useful information (since the rider never finished or fouled), these entries will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rnk'] = pd.to_numeric(df['Rnk'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PI 7: Part 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Appropriate Machine Learning Models\n",
    "In this section, the selection of supervised machine learning models in the context of the current dataset will be discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Random Forest\n",
    "*Random forest* is an ensemble method, that makes use of bootstrapping catergorical and regression trees (cart). As stated in *part 2* of the assignment, cart is a decision tree that classifies records based on the conditions in the *decision nodes*, where the final classification, or regression, is determined in the *leaf nodes*. See the code from <code>pi7-2.ipynb</code> for a more in-depth explanation of decision trees.\n",
    "\n",
    "\n",
    "**What is ensemble learning?**\n",
    "- Ensemble learning is a model that will construct a number of models, with the aim to improve its prediction rate.\n",
    "Two popular ensemble methods include *bagging* (Bootstrap Aggregation) and *boosting*. With bagging, weak models are trained in parallel, where each model learns from a given partition of the dataset. The results from each of these models are then combined by either averaging (regression) or voting (classification). With boosting, a sequential method, each model is trained on a weighted training set, where each model corrects the weights based on the error from its predecessors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest method will, with regression, create multiple decision trees during training, where the output is the average prediction of the trees.\n",
    "Standalone trees are prone to high variance and overgrowing, resulting in overfitting. Random forest aims to overcome this by selecting a random set of features for a decision tree, where each tree will make a prediction based on the given features. In the final step, all predictions from the trees are averaged (regression). This provides a degree of randomness for the model, gaining the benefit of:\n",
    "- Reducing overfitting: by using different subsets of features, this model reduces the risk of training on only a specific subset specific patterns\n",
    "in the dataset.\n",
    "- Generalization: by combining predictions from multiple trees, a wider range of patterns is captured in the dataset, which makes the model moder generalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3 Implementation\n",
    "Below is an implementation of random forest on the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe with only numericals, because regression\n",
    "# Variable selection will be done later\n",
    "df_ml = df.select_dtypes(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 80174 entries, 659ed585810c65fe22255a5e4a9b7838 to 0292146b9196ec7a98903cb50dae48cd\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Rnk              74955 non-null  float64\n",
      " 1   GC               70115 non-null  float64\n",
      " 2   Age              80174 non-null  int64  \n",
      " 3   UCI              2647 non-null   float64\n",
      " 4   Pnt              10407 non-null  float64\n",
      " 5   Length           73679 non-null  float64\n",
      " 6   Month            80174 non-null  int32  \n",
      " 7   Year             80174 non-null  int32  \n",
      " 8   Time_seconds     61739 non-null  float64\n",
      " 9   Timelag_seconds  68962 non-null  float64\n",
      " 10  Stage_Type_bin   7690 non-null   float64\n",
      " 11  height           45914 non-null  float64\n",
      " 12  weight           45908 non-null  float64\n",
      " 13  PCS_Rnk          34332 non-null  float64\n",
      " 14  UCI_Rnk          33370 non-null  float64\n",
      " 15  AllTime_Rnk      26874 non-null  float64\n",
      " 16  Day_Pnt          45958 non-null  float64\n",
      " 17  GC_Pnt           45966 non-null  float64\n",
      " 18  TT_Pnt           45543 non-null  float64\n",
      " 19  Sprint_Pnt       45832 non-null  float64\n",
      " 20  Climb_Pnt        45809 non-null  float64\n",
      "dtypes: float64(18), int32(2), int64(1)\n",
      "memory usage: 12.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_ml.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datapoints from the lower 20% and upper 80% are removed from the dataset. This reduces the size of the dataset significantly, making training\n",
    "computationally more feasible in the later stages and will remove any outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_low = df_ml.quantile(0.2)\n",
    "q_hi = df_ml.quantile(0.8)\n",
    "\n",
    "df_filtered = df_ml[(df_ml < q_hi) & (df_ml > q_low)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing entries will be filled using the mean value, making more data available for training. Mean was chosen, since this method is \n",
    "computationally inexpensive, making the process more feasible timewise. This does degrade accuracy in finding underlying patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th0miz/Documents/School/DatSci_PI7-1/.venv/lib/python3.13/site-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Stage_Type_bin']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imp = SimpleImputer(strategy='mean').set_output(transform='pandas')\n",
    "\n",
    "imp.fit(df_filtered)\n",
    "df_filtered = imp.transform(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 80174 entries, 659ed585810c65fe22255a5e4a9b7838 to 0292146b9196ec7a98903cb50dae48cd\n",
      "Data columns (total 20 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Rnk              80174 non-null  float64\n",
      " 1   GC               80174 non-null  float64\n",
      " 2   Age              80174 non-null  float64\n",
      " 3   UCI              80174 non-null  float64\n",
      " 4   Pnt              80174 non-null  float64\n",
      " 5   Length           80174 non-null  float64\n",
      " 6   Month            80174 non-null  float64\n",
      " 7   Year             80174 non-null  float64\n",
      " 8   Time_seconds     80174 non-null  float64\n",
      " 9   Timelag_seconds  80174 non-null  float64\n",
      " 10  height           80174 non-null  float64\n",
      " 11  weight           80174 non-null  float64\n",
      " 12  PCS_Rnk          80174 non-null  float64\n",
      " 13  UCI_Rnk          80174 non-null  float64\n",
      " 14  AllTime_Rnk      80174 non-null  float64\n",
      " 15  Day_Pnt          80174 non-null  float64\n",
      " 16  GC_Pnt           80174 non-null  float64\n",
      " 17  TT_Pnt           80174 non-null  float64\n",
      " 18  Sprint_Pnt       80174 non-null  float64\n",
      " 19  Climb_Pnt        80174 non-null  float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 12.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X represents the predictor values (row 1 to n), y represents the target values (ranking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.iloc[:, 1:]\n",
    "y = df_filtered.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 80174 entries, 659ed585810c65fe22255a5e4a9b7838 to 0292146b9196ec7a98903cb50dae48cd\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   GC               80174 non-null  float64\n",
      " 1   Age              80174 non-null  float64\n",
      " 2   UCI              80174 non-null  float64\n",
      " 3   Pnt              80174 non-null  float64\n",
      " 4   Length           80174 non-null  float64\n",
      " 5   Month            80174 non-null  float64\n",
      " 6   Year             80174 non-null  float64\n",
      " 7   Time_seconds     80174 non-null  float64\n",
      " 8   Timelag_seconds  80174 non-null  float64\n",
      " 9   height           80174 non-null  float64\n",
      " 10  weight           80174 non-null  float64\n",
      " 11  PCS_Rnk          80174 non-null  float64\n",
      " 12  UCI_Rnk          80174 non-null  float64\n",
      " 13  AllTime_Rnk      80174 non-null  float64\n",
      " 14  Day_Pnt          80174 non-null  float64\n",
      " 15  GC_Pnt           80174 non-null  float64\n",
      " 16  TT_Pnt           80174 non-null  float64\n",
      " 17  Sprint_Pnt       80174 non-null  float64\n",
      " 18  Climb_Pnt        80174 non-null  float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A train-test split is also made, where the models are able to train on 80% of the dataset, and tested/validated on the remaining 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actual implement\n",
    "Below is the actual implementation of the random forest regression model. A pipeline is set up to enable feature selection in a more concise statement, making the code more concise. Feature selection is performed by first training a model to extract the most important variables in the first model. Next, those captured variables are used for training the actual model. This makes training more complex timewise, but it does seem to improve accuracy rating and pattern capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline test score: 0.30190076820878364\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestRegressor(n_estimators=100)\n",
    "forest.fit(X_test, y_test)\n",
    "\n",
    "\n",
    "# Hyperparameters are adjusted to findings from random search cv\n",
    "pipeline_forest = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(estimator=forest)),\n",
    "    ('forest_regression', RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5))\n",
    "])\n",
    "\n",
    "pipeline_forest.fit(X_train, y_train)\n",
    "\n",
    "score = pipeline_forest.score(X_test, y_test)\n",
    "print(f\"Pipeline test score: {score}\")\n",
    "\n",
    "y_pred_dtr = pipeline_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Histogram-based Gradient Boosting Regression\n",
    "\n",
    "\n",
    "In the following section, *histogram-based gradient boosting regression* will be discussed. \n",
    "\n",
    "This machine learning model makes use of *gradient boosting* and the construction of histogram-based features.\n",
    "\n",
    "*Gradient boosting* is an ensemble method which makes use of regression trees. This is a boosting method, which works by building multiple, weaker, prediction models in sequence. Each time a model is constructed, it will (try to) predict the error that is left over by its model, and uses this information to build the next model. Gradient boosting makes use of combining predictions of multiple weak learners, usually decision trees.\n",
    "\n",
    "\n",
    "**How does Hist Gradient Boosting work?**\n",
    "This model is similar to gradient boosting, as it is an ensemble technique which uses boosting, but instead of using decision trees as weak learners, it makes use of histograms. Using histograms is computationally more efficient, making it faster compared to traditional gradient boosting, especially on larger datasets. Building a histogram has a notation of $O(nfeatures​×n)$, whereas gradient boosting is of $O(nfeatures​×nlog(n))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the following advantages:\n",
    "- Faster training times.\n",
    "- It can handle larger datasets.\n",
    "- Data does not need to be normalized.\n",
    "- Histogram-based can handle non-linear interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.3 Implementation\n",
    "See below for the implementation of histogram-based gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16035, 19), (16035,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline does not perform any special actions in this case\n",
    "pipeline_hgb = Pipeline([\n",
    "    ('hgb', HistGradientBoostingRegressor(loss='poisson', max_bins=255, max_iter=300, max_depth=10, learning_rate=0.2, ))\n",
    "])\n",
    "\n",
    "pipeline_hgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_hgb = pipeline_hgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Support Vector Regression (SVR)\n",
    "In the following section, SVR will be discussed and showcased.\n",
    "\n",
    "SVR is a type of learning model, found in the set of models from Support Vector Machines (SVM). SVM is a supervised model that can handle both unstructured data as well as nonlinear relationships in high dimensional spaces. This is made possible due to a nifty feature of this type of model: the *kernel trick* or also called *kernel method*.\n",
    "\n",
    "\n",
    "Svr works by mapping the 2D space, wherein the samples and target values live, to a higher dimension using the *kernel trick*. It will then\n",
    "map a hyperplane in this, now multidimensional, space where the data is present. The goal for the usage of this hyperplane is to find the space where the least amount of error is present between the predicted values and the actual values. This hyperplane will then be used for making new predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel trick is the main property that gives svm the ability to use a higher-dimensional space. It works by mapping the datapoints into a higher-dimensional space, where the use of a *kernel function* computes the similarity between two points in the original space. There are several algorithms for performing this kernel function, like *linear kernel*, *polynomial kernel* and *radial basis function (RBF) kernel*. So, the kernel function will calculate similarity between datapoints in the original space, and will in turn calculate dot products, based on these similarities, in the higher dimensional space. The svr algorithm will then find a hyperplane between the dot products, where the error between predicted values and actual values are minimal. This hyperplane is then used for making predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another pipeline setup to standardize the data\n",
    "# before performing svr on the data\n",
    "# RBF kernel method is used.\n",
    "\n",
    "pipeline_svr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf', C=1.0, epsilon=0.2))\n",
    "])\n",
    "\n",
    "pipeline_svr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svr = pipeline_svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Metrics for Models\n",
    "\n",
    "In the following section, metrics for analyzing the performance of the used models will be explained and demonstrated.\n",
    "\n",
    "Since regression models are being used, the following general metrics will be used:\n",
    "- Mean absolute error (MAE) and mean absolute percentage error (MAPE).\n",
    "- Coefficient of determination ($R^2$).\n",
    "\n",
    "For random forest, feature importance will be used as a metric for evaluating the amount times a feature is used in each tree of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Mean Absolute (Percentage) Error\n",
    "\n",
    "\n",
    "With MAE, the magnitude of the average absolute error, or also called deviation, is determined.\n",
    "For MAPE, the same is achieved as with MAE, except it will return this as a precentage. Thus, this measure gives a percentage score of how predictions deviate from the actual values. \n",
    "\n",
    "Below, the MAE and MAPE for each of the models will be given, and will be interpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest regression:\n",
      "\n",
      "Mean absolute error:  11.718033131994183 \n",
      "Mean absolute percentage error:  0.17351675614395565 \n",
      "\n",
      "Histogram-based gradient boosting:\n",
      "Mean absolute error:  12.175877989606153 \n",
      "Mean absolute percentage error:  0.1803830237454865 \n",
      "\n",
      "Support vector regression:\n",
      "Mean absolute error:  12.60602342048053 \n",
      "Mean absolute percentage error:  0.19033841213374497\n"
     ]
    }
   ],
   "source": [
    "mae_dtr = mean_absolute_error(y_test, y_pred_dtr)\n",
    "mape_dtr = mean_absolute_percentage_error(y_test, y_pred_dtr)\n",
    "\n",
    "mae_hgb = mean_absolute_error(y_test, y_pred_hgb)\n",
    "mape_hgb = mean_absolute_percentage_error(y_test, y_pred_hgb)\n",
    "\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
    "mape_svr = mean_absolute_percentage_error(y_test, y_pred_svr)\n",
    "\n",
    "print(\"Random forest regression:\\n\\nMean absolute error: \", mae_dtr, \"\\nMean absolute percentage error: \", mape_dtr,\n",
    "      \"\\n\\nHistogram-based gradient boosting:\\nMean absolute error: \", mae_hgb, \"\\nMean absolute percentage error: \", mape_hgb,\n",
    "      \"\\n\\nSupport vector regression:\\nMean absolute error: \", mae_svr, \"\\nMean absolute percentage error: \", mape_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our metrics, the following can be noted:\n",
    "\n",
    "\n",
    "For mean absolute error, which means the average distance that the model was off from making a correct prediction on an unseen dataset, is \n",
    "the lowest for random tree regression, where it was off by 11 points on average. It also has the lowest rate for mean absolute percentage error, where it was off by around 17,4% on average. Histogram-based gradient boosting and support vector regression had about the same results, but they both scored lower on these metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Coefficient Determination\n",
    "Coefficient determination, or also called a $R^2$ score, is a measure that evaluates the goodness of a fit of a regression model. With this score, the proportion to which a regression model adapted to the underlying relations between predictor and target variables is expressed, with a number between 0 and 1. The lower the score, the less a model was able to explain the variation in the dependent variable from its independent variables and vice versa. So, a low score that is closer to 0 explains that the model is a poor fit to the data and that the independent variables do not explain much of the variation to the dependent variable. For a higher score closer to 1, the model will seem like a good fit to the data, and that the independent variables explain most of the variation in the dependent variable. This method works best when variables have linear relationships.\n",
    "\n",
    "$R^2$ score should not be used as the only metric for a model, since it does not explicitly score the predictive power of a trained model, but it should be used as a metric in combination with other metrics, such as mean absolute error for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " . . R2 scores . . \n",
      "\n",
      "R^2 for random forest:  0.28729673106258613 \n",
      "R^2 for histogram-based gradient boost:  0.22437971684893832 \n",
      "R^2 for support vector regression:  0.11078191543273053\n"
     ]
    }
   ],
   "source": [
    "r2_dtr = r2_score(y_test, y_pred_dtr)\n",
    "r2_hgb = r2_score(y_test, y_pred_hgb)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(\" . . R2 scores . . \\n\\nR^2 for random forest: \", r2_dtr,\n",
    "      \"\\nR^2 for histogram-based gradient boost: \", r2_hgb,\n",
    "      \"\\nR^2 for support vector regression: \", r2_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, random forest was able to capture most of the underlying relationships between dependent and independent variables, compared to random forest and svr. It was able to explain around 28,7% of the variation in the predicted ranking of a cyclist, that was explained by the characteristics of the given cyclists. Histogram-based gradient boosting comes in second and svr comes in last.\n",
    "\n",
    "Given the $R^2$ scores, it can be concluded that the supervised learning models have a rather poor fit, since a score of 0.5 and up would indicate better predictive power. This could be due to a number reasons, such as overfitting due to the preprocessing stage or the fact that relationships are non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 Feature Importance\n",
    "Below are the features that random forest deemed as the most significant/the strongest predictor values that have the most influence on the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = np.argsort(importances)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(range(len(importances)), importances[indices])\n",
    "ax.set_yticks(range(len(importances)))\n",
    "_ = ax.set_yticklabels(np.array(X_train.columns)[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, it can be concluded that 'GC' was deemed as the most important value for making decisions. This could be due to the fact that it is actually a good predictor value, or it could be a result of imputating by the mean during preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Cross-validation\n",
    "\n",
    "Cross validation (cv) is the act of splitting the dataset, that will be used for the training of a machine learning model.\n",
    "In this section, the results of research about the preparation of data for cross-validation will be discussed.\n",
    "\n",
    "Lets start off by evaluating the meaning of cross validation.\n",
    "\n",
    "When an entire dataset is both used for training and testing the model, it will be very accurate, since the model will use the memorized labels during training for testing its own performance. This is also called *overfitting*, where the model memorized the data instead of learning from it. Thus, it is good practice to hold out a part of the dataset for testing, since the model has never used this partition of the dataset for training purposes. Cv takes this a step further, by splitting the dataset into a training and a testing partition. With this, multiple instances of a model will be trained, where each model will have the test partition shifted around. By iterating over different splits during each sequentially trained model, a more accurate claim can be made for the performance of the trained model. See below for a visual representation.\n",
    "\n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "\n",
    "[Scikit-learn](http://example.com)\n",
    "\n",
    "\n",
    "With this, the basics of *k-fold cross-validation* (k-cv), and thus cv itself, have been outlined, where the training set is split into *k* smaller sets, called \"k folds\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 Cross Validation for Hyperparameters\n",
    "\n",
    "However, the data used for training is not the only reason the prediction power of a model can be influenced. ML models are tuned based on *hyperparameters* to control their behaviour. For instance, the prediction power for a random forest model can rely on the maximum depth of the tree, the minimum number of samples require to allow a split and more. To dive deeper into researching what hyperparameters work best for a given model, cross validation can be used to have a certain model train over the dataset, while iterating through the definitions for hyperparameters. The results from cross validation can be interpreted to make a claim over which hyperparameters work best for a given model.\n",
    "\n",
    "So in short, it can be stated that the hyperparameter space can be iteratively searched over for the best cross validation score. But how are hyperparameters evaluated? Two methods will be discussed below.\n",
    "\n",
    "1. **Exhaustive grid search**\n",
    "\n",
    "With a grid search, the hyperparameters are specified from a grid of parameter values. The values that will be evaluated are predefined in a grid, where in each following evaluation, the next hyperparameter value is evaluated. By doing this, all the possible combinations of predefined hyperparameters are evaluated, which will result in the best combination of parameters that are defined in the grid. This method is one of the more widely used methods for validating the most optimal hyperparameters for a machine learning model.\n",
    "\n",
    "\n",
    "2. **Randomized parameter optimization**\n",
    "\n",
    "Randomized parameter optimization takes a different approach to finding the ideal hyperparameters. This method implements a randomized search over the parameters for a model, where each setting is sampled over a range of possible parameters. The main goal of this cv method is to find the best model parameters by training and evaluating the model, see the next chapter for an implementation of randomized search cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Ideal Hyperparameters\n",
    "\n",
    "Below is an implementation of *randomized parameter optimization*, or also call *randomized search cross validation*. This will be done for each of the used models: random forest, histogram-based gradient boosting and support vector regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 100, 'min_samples_split': 10, 'max_depth': None}\n",
      "Best Score: 0.18289787577056232\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Random Forest\n",
    "\"\"\"\n",
    "\n",
    "# Define hyperparameter grid for RandomForest Classifier\n",
    "param_grid = { 'n_estimators': [50, 100, 200],\n",
    "              'max_depth': [None, 5, 10],\n",
    "              'min_samples_split': [2, 5, 10] }\n",
    "\n",
    "rf_classifier = RandomForestRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier,\n",
    "                                   param_distributions=param_grid,\n",
    "                                   cv=5,\n",
    "                                   n_iter=10)\n",
    "\n",
    "random_search.fit(X_test, y_test)\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB Best Hyperparameters: {'max_iter': 300, 'max_depth': None, 'learning_rate': 0.2, 'l2_regularization': 1.0}\n",
      "HGB Best Score: 0.27361037590434323\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Histogram-based Gradient Boosting\n",
    "\"\"\"\n",
    "param_grid_hgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'l2_regularization': [0, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "hgb = HistGradientBoostingRegressor()\n",
    "\n",
    "random_search_hgb = RandomizedSearchCV(estimator=hgb,\n",
    "                                       param_distributions=param_grid_hgb,\n",
    "                                       cv=5,\n",
    "                                       n_iter=10)\n",
    "\n",
    "random_search_hgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"HGB Best Hyperparameters:\", random_search_hgb.best_params_)\n",
    "print(\"HGB Best Score:\", random_search_hgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSupport Vector Regression\\n\\nparam_grid_svr = {\\n    \\'C\\': [0.1, 1.0, 10],\\n    \\'epsilon\\': [0.1, 0.2, 0.5],\\n    \\'kernel\\': [\\'linear\\', \\'poly\\', \\'rbf\\']\\n}\\n\\nsvr = SVR()\\n\\nrandom_search_svr = RandomizedSearchCV(estimator=svr,\\n                                       param_distributions=param_grid_svr,\\n                                       cv=5,\\n                                       n_iter=10)\\n\\nrandom_search_svr.fit(X_train, y_train)\\n\\nprint(\"SVR Best Hyperparameters:\", random_search_svr.best_params_)\\nprint(\"SVR Best Score:\", random_search_svr.best_score_) '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Support Vector Regression\n",
    "\"\"\"\n",
    "\n",
    "param_grid_svr = {\n",
    "    'C': [0.1, 1.0, 10],\n",
    "    'epsilon': [0.1, 0.2, 0.5],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "random_search_svr = RandomizedSearchCV(estimator=svr,\n",
    "                                       param_distributions=param_grid_svr,\n",
    "                                       cv=5,\n",
    "                                       n_iter=10)\n",
    "\n",
    "random_search_svr.fit(X_train, y_train)\n",
    "\n",
    "print(\"SVR Best Hyperparameters:\", random_search_svr.best_params_)\n",
    "print(\"SVR Best Score:\", random_search_svr.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 Interpretation\n",
    "For the random forest regression model, the optimal hyperparameters are as follows:\n",
    "- <code>n_estimators</code>: this indicates the number of trees present in the \"forest\". The optimal number is 100.\n",
    "- <code>min_samples_split</code>: this is the number of samples required to split a node. The optimal number is 5.\n",
    "- <code>max_depth</code>: this is the amount of depth a tree has, so it controls the depth of the tree. The optimal number is 'none' for no max depth.\n",
    "\n",
    "\n",
    "For the histogram-based gradient boost regression model, the optimal hyperparameters are as follows:\n",
    "- <code>max_iter</code>: this is the maximum amount of iterations during the training process. The optimal number is 300.\n",
    "- <code>max_depth</code>: this is the amount of depth a tree has, so it controls the depth of the tree. The optimal number is 10.\n",
    "- <code>learning_rate</code>: this represents the speed at which the model learns. The optimal number is 0.2\n",
    "- <code>l2_regularization</code>: this number represents the extent to which a measure is taken to prevent overfitting, in short. The optimal number was 0 for no L2 regularization.\n",
    "\n",
    "\n",
    "For SVR, it was not possible to perform random search cross validation, due to computational overhead. The hyperparameters will be explained in short, but no conclusion can be given on the perfect value.\n",
    "- <code>C</code>: this parameter sets the strength of regularization. Regularization is a technique of helping to mitigate overfitting.\n",
    "- <code>epsilon</code>: this is associated to how random an action is taken. To put it simply, it trains the model with the aim to be around the same values as predictors, but not to be the exact value of these predictors.\n",
    "- <code>Kernel</code>: this specifies the type of kernel method that is used in the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score indicates the average of $R^2$ scores over the left-out test fold for model using the most optimal parameters.\n",
    "For the random forest model, the score was around 0,177. For histogram-based gradient boosting model, the score is around 0,277."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Conclusions\n",
    "To make a conclusion from the research that was performed, the data from the interpretation of the optimal hyperparameters will be used. Since the $R^2$ score is the highest for the histogram-based gradient boosting model, meaning that this model captures the underlying patterns in the dataset best. When this is the case, the model is able to more accurately identify relationships between the features and a target. With this, the model is able to make a better prediction of the target value.\n",
    "\n",
    "With an $R^2$ value of around 0,277, it would mean that the model learned some of those underlying patterns, but it is still missing out on most of those important relationships. This could be due to a couple of reasons: an incomplete amount of features, where the key inputs for the target value are missing; there was too much noise present in the dataset, meaning that the preprocessing needs improvement; the model might just be too complex, to the point where the model could be overfitting.\n",
    "\n",
    "The $R^2$ score is not the only metric that should be used for making a conclusion on the performance of a model, since it does not explicitly outline its overall predictive power. For instance, with the ideal hyperparameters found from random search cv, the mean absolute error is lowest for random forest, with a value of around 11. This means that random forest is, on average, off by around 11 ranking points when validating on new data. Compared to histogram-based gradient boosting, which sits around 12 for MAE, using random forest would be more accurate overall.\n",
    "\n",
    "Taking this into account, the conclusion can be made that a histogram-based gradient boosting regressor would be a model that captures underlying patters better than random forest or svr could, with a slight compromise in overall accuracy. Given this result, histogram-based gradient boosting would be the best solution to training a supervised machine learning model that can help predict the ranking of a given cyclist in their next match, given that the dataset has been properly sanitized and that the important predictor values have been identified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bibliography:\n",
    "Chicco, D., Warrens, M. J., & Jurman, G. (2021). The Coefficient of Determination R-squared Is More Informative than SMAPE, MAE, MAPE, MSE and RMSE in Regression Analysis Evaluation. *PeerJ Computer Science, 7(5)*, e623. ncbi. https://doi.org/10.7717/peerj-cs.623\n",
    "\n",
    "\n",
    "*Cross-validation: Evaluating Estimator Performance*. (2009). Scikit-Learn.org. https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "\n",
    "‌*Ensembles: Gradient boosting, random forests, bagging, voting, stacking*. (2022). Scikit-Learn. \n",
    "    https://scikit-learn.org/stable/modules/ensemble.html#gradientboostingclassifier-and-gradientboostingregressor\n",
    "\n",
    "\n",
    "*HistGradientBoostingRegressor*. (n.d.). Scikit-Learn. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html\n",
    "\n",
    "*Interpreting sklearns’ GridSearchCV best score*. (2018). Stack Overflow. https://stackoverflow.com/questions/50232599/interpreting-sklearns-gridsearchcv-best-score\n",
    "\n",
    "\n",
    "*Support Vector Regression (SVR)*. (2024). Medium. https://farshadabdulazeez.medium.com/support-vector-regression-svr-8cb1966ff5a0\n",
    "\n",
    "\n",
    "*Support Vector Regression - Coastal Wiki.* (2024). Coastalwiki.org. https://www.coastalwiki.org/wiki/Support_Vector_Regression\n",
    "\n",
    "‌\n",
    "*The Kernel Trick in Support Vector Machine (SVM).* (2022, May 9).  YouTube. https://youtu.be/Q7vT0--5VII?si=UPm5UYwL1Dtsgkhw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
