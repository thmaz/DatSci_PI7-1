{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sqlite3 as lite\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of rows in races_df:  225918\n",
      "Amount of rows in riders_df:  1042\n",
      "Unique values for 'months':  [ 1  3  4  5  6  7  8  9 10] \n",
      "\n",
      "Unique values for 'years':  [2012 2014 2015 2017 2018 2020 2021]\n",
      "  Timelag  Timelag_seconds\n",
      "0   +0:00              NaN\n",
      "1   +0:04              4.0\n",
      "2   +0:06              6.0\n",
      "3   +0:10             10.0\n",
      "4   +0:10             10.0\n",
      "5   +0:10             10.0\n",
      "6   +0:10             10.0\n",
      "7   +0:10             10.0\n",
      "8   +0:10             10.0\n",
      "9   +0:10             10.0\n"
     ]
    }
   ],
   "source": [
    "conn = lite.connect('cycling_big.db')\n",
    "\n",
    "riders_df = pd.read_sql_query('SELECT * FROM riders;', conn)\n",
    "races_df = pd.read_sql_query('SELECT * FROM race_results', conn)\n",
    "\n",
    "print(\"Amount of rows in races_df: \", races_df[races_df.columns[0]].count())\n",
    "print(\"Amount of rows in riders_df: \", riders_df[riders_df.columns[0]].count())\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\"\"\"\n",
    "Convert the 'Date' column to DateTime format\n",
    "Regarding races_df dataframe!\n",
    "\"\"\"\n",
    "\n",
    "races_df['Date'] = pd.to_datetime(races_df['Date'], errors='coerce', format='%d %B %Y')\n",
    "\n",
    "# Remove rows where 'Date' is NaT\n",
    "races_df = races_df.dropna(subset=['Date'])\n",
    "\n",
    "# Normalize 'Date' to strip out time if it's present (this keeps just the date part)\n",
    "races_df['Date'] = races_df['Date'].dt.normalize()\n",
    "\n",
    "# Extract the month and year from the column and put them in their own columns\n",
    "races_df['Month'] = races_df['Date'].dt.month\n",
    "races_df['Year'] = races_df['Date'].dt.year\n",
    "\n",
    "print(\"Unique values for 'months': \", races_df['Month'].unique(), \"\\n\")\n",
    "print(\"Unique values for 'years': \",races_df['Year'].unique())\n",
    "\n",
    "\"\"\"\n",
    "Converting the timetable to total seconds\n",
    "\"\"\"\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    # Remove commas and any spaces\n",
    "    time_str = time_str.replace(',', '').strip()\n",
    "\n",
    "    # Check the consistency of the time format using regular expressions\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2}):(\\d{2})', time_str)\n",
    "    if match:\n",
    "        hours, minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "        if total_seconds == 0:\n",
    "            return np.nan\n",
    "        return total_seconds\n",
    "\n",
    "    # Do the same as the loop above, but now for MM:SS format.\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2})', time_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        if total_seconds == 0:\n",
    "            return np.nan  \n",
    "        return total_seconds\n",
    "\n",
    "    # Do the same but for 0:00, 0:01, etc.\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{1,2})', time_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        if total_seconds == 0:\n",
    "            return np.nan \n",
    "        return total_seconds\n",
    "    \n",
    "    # If format doesn't match, return NaN\n",
    "    return np.nan\n",
    "\n",
    "# Apply the conversion function to the 'Time' column\n",
    "races_df['Time_seconds'] = races_df['Time'].apply(time_to_seconds)\n",
    "\n",
    "# print(races_df[['Time', 'Time_seconds']].head(10))\n",
    "# races_df.info()\n",
    "\n",
    "\"\"\"\n",
    "Converting timelag to total seconds\n",
    "\"\"\"\n",
    "\n",
    "def timelag_to_seconds(timelag_str):\n",
    "    # Check if the timelag_str contains missing values\n",
    "    if pd.isna(timelag_str):\n",
    "        return np.nan\n",
    "\n",
    "    timelag_str = timelag_str.lstrip('+').strip()\n",
    "\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2}):(\\d{2})', timelag_str)\n",
    "    if match:\n",
    "        hours, minutes, seconds = map(int, match.groups())\n",
    "        return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2})', timelag_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        return minutes * 60 + seconds\n",
    "    \n",
    "    match = re.match(r'(\\d+):(\\d+)', timelag_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        return minutes * 60 + seconds\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "races_df['Timelag_seconds'] = races_df['Timelag'].apply(timelag_to_seconds)\n",
    "\n",
    "races_df['Timelag_seconds'] = races_df['Timelag_seconds'].replace(0.0, np.nan)\n",
    "\n",
    "print(races_df[['Timelag', 'Timelag_seconds']].head(10))\n",
    "\n",
    "\"\"\"\n",
    "Converting distance into single numerical value\n",
    "This means stripping 'km' from string and converting the remaining values into float64\n",
    "\"\"\"\n",
    "\n",
    "races_df['Length'] = races_df['Length'].str.replace(' km', '', regex=False)\n",
    "\n",
    "# Convert to numeric and replace 0 with NaN\n",
    "races_df['Length'] = pd.to_numeric(races_df['Length'], errors='coerce')  # Convert to numeric and handle errors\n",
    "\n",
    "# Replace 0 values with NaN\n",
    "races_df['Length'] = races_df['Length'].replace(0.0, np.nan)\n",
    "\n",
    "# print(races_df['Length'])\n",
    "\n",
    "\"\"\"\n",
    "Splitting values from 'rdr' and putting the split values into separate columns\n",
    "\"\"\"\n",
    "\n",
    "# Function to convert the 'rdr' string to separate ranking columns\n",
    "def extract_rankings(rdr_str):\n",
    "    try:\n",
    "        # Converting str to dict\n",
    "        rankings = ast.literal_eval(rdr_str)\n",
    "        \n",
    "        # Extracting ranks, while also handling missing keys\n",
    "        pcs_rnk = rankings.get('PCS Ranking', np.nan)\n",
    "        uci_rnk = rankings.get('UCI World Ranking', np.nan)\n",
    "        alltime_rnk = rankings.get('Specials | All Time Ranking', np.nan)\n",
    "        \n",
    "        return pd.Series([pcs_rnk, uci_rnk, alltime_rnk])\n",
    "    except:\n",
    "        return pd.Series([np.nan, np.nan, np.nan])\n",
    "\n",
    "# Apply the function to the 'rdr' column\n",
    "riders_df[['PCS_Rnk', 'UCI_Rnk', 'AllTime_Rnk']] = riders_df['rdr'].apply(extract_rankings)\n",
    "\n",
    "# Entries should be turned into numeric values, where errors get turned into NaN\n",
    "riders_df['PCS_Rnk'] = pd.to_numeric(riders_df['PCS_Rnk'], errors='coerce')\n",
    "riders_df['UCI_Rnk'] = pd.to_numeric(riders_df['UCI_Rnk'], errors='coerce')\n",
    "riders_df['AllTime_Rnk'] = pd.to_numeric(riders_df['AllTime_Rnk'], errors='coerce')\n",
    "\n",
    "# print(riders_df[['fullname', 'PCS_Rnk', 'UCI_Rnk', 'AllTime_Rnk']])\n",
    "\n",
    "\"\"\"\n",
    "Convert stage types to binary with label encoding\n",
    "\"\"\"\n",
    "\n",
    "races_df['Stage_Type_bin'] = races_df['Stage_Type'].map({'RR': 0, 'ITT': 1})\n",
    "\n",
    "\"\"\"\n",
    "Splitting values from 'pps' and putting those values into separate columns.\n",
    "\"\"\"\n",
    "\n",
    "def extract_points(pps_str):\n",
    "    try:\n",
    "        points = ast.literal_eval(pps_str)\n",
    "        \n",
    "        day_pnt = np.nan if points.get('One day races', '0') == '0' else points.get('One day races', np.nan)\n",
    "        gc_pnt = np.nan if points.get('GC', '0') == '0' else points.get('GC', np.nan)\n",
    "        tt_pnt = np.nan if points.get('Time trial', '0') == '0' else points.get('Time trial', np.nan)\n",
    "        sprint_pnt = np.nan if points.get('Sprint', '0') == '0' else points.get('Sprint', np.nan)\n",
    "        climb_pnt = np.nan if points.get('Climber', '0') == '0' else points.get('Climber', np.nan)\n",
    "        \n",
    "        return pd.Series([day_pnt, gc_pnt, tt_pnt, sprint_pnt, climb_pnt])\n",
    "    except:\n",
    "        return pd.Series([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "riders_df[['Day_Pnt', 'GC_Pnt', 'TT_Pnt', 'Sprint_Pnt', 'Climb_Pnt']] = riders_df['pps'].apply(extract_points)\n",
    "\n",
    "riders_df['Day_Pnt'] = pd.to_numeric(riders_df['Day_Pnt'], errors='coerce')\n",
    "riders_df['GC_Pnt'] = pd.to_numeric(riders_df['GC_Pnt'], errors='coerce')\n",
    "riders_df['TT_Pnt'] = pd.to_numeric(riders_df['TT_Pnt'], errors='coerce')\n",
    "riders_df['Sprint_Pnt'] = pd.to_numeric(riders_df['Sprint_Pnt'], errors='coerce')\n",
    "riders_df['Climb_Pnt'] = pd.to_numeric(riders_df['Climb_Pnt'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          BARDET Romain\n",
      "1        DUMOULIN Samuel\n",
      "2          GALLOPIN Tony\n",
      "3          NAESEN Oliver\n",
      "4          FRANK Mathias\n",
      "              ...       \n",
      "1037      TRONDSEN Trond\n",
      "1038    VAN MELSEN Kévin\n",
      "1039     BEULLENS Cédric\n",
      "1040    DE WINTER Ludwig\n",
      "1041      DELACROIX Théo\n",
      "Name: fullname, Length: 1042, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(riders_df['fullname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          BARDET_Romain\n",
      "1        DUMOULIN_Samuel\n",
      "2          GALLOPIN_Tony\n",
      "3          NAESEN_Oliver\n",
      "4          FRANK_Mathias\n",
      "              ...       \n",
      "1037      TRONDSEN_Trond\n",
      "1038    VAN_MELSEN_Kévin\n",
      "1039     BEULLENS_Cédric\n",
      "1040    DE_WINTER_Ludwig\n",
      "1041      DELACROIX_Théo\n",
      "Name: fullname, Length: 1042, dtype: object\n"
     ]
    }
   ],
   "source": [
    "riders_df['fullname'] = riders_df['fullname'].str.replace(' ', '_') \n",
    "print(riders_df['fullname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of rows in races_df post cleanup:  80174\n",
      "Amount of rows in riders_df post cleanup:  1042\n",
      "Amount of rows in df:  45966\n",
      "Rnk                    0\n",
      "GC                  5600\n",
      "BiB                    0\n",
      "Rider                  0\n",
      "Age                    0\n",
      "UCI                44040\n",
      "Pnt                39107\n",
      "Length              5484\n",
      "Month                  0\n",
      "Year                   0\n",
      "Time_seconds       10650\n",
      "Timelag_seconds     6257\n",
      "Stage_Type_bin     41132\n",
      "fullname               0\n",
      "team                   0\n",
      "country                0\n",
      "height                52\n",
      "weight                58\n",
      "PCS_Rnk            11634\n",
      "UCI_Rnk            12596\n",
      "AllTime_Rnk        19092\n",
      "Day_Pnt                8\n",
      "GC_Pnt                 0\n",
      "TT_Pnt               423\n",
      "Sprint_Pnt           134\n",
      "Climb_Pnt            157\n",
      "dtype: int64\n",
      "Amount of rows in df after cleanup:  45966\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Join the rider and race tables together, using the rider_id as an index\n",
    "\"\"\"\n",
    "print(\"Amount of rows in races_df post cleanup: \", races_df[races_df.columns[0]].count())\n",
    "print(\"Amount of rows in riders_df post cleanup: \", riders_df[riders_df.columns[0]].count())\n",
    "\n",
    "df = races_df.set_index('rider_id').join(riders_df.set_index('rider_id'), how = 'inner')\n",
    "\n",
    "print(\"Amount of rows in df: \", df[df.columns[0]].count())\n",
    "\n",
    "\"\"\"\n",
    "Dropping columns that are not needed for analysis\n",
    "\"\"\"\n",
    "\n",
    "# Note: fix the long list\n",
    "df.drop(['Time', 'Timelag', 'rdr', 'pps', 'birthdate', 'rider_url', 'Race_url', 'Stage_url', 'Circuit', 'Race_Name', 'Stage_Name', 'Start', 'Finish', 'Category', 'Stage_Type'], axis=1, inplace=True)\n",
    "\n",
    "# Additional drops:\n",
    "df = df.drop(columns=['id', 'Team', 'Date', 'Race_ID', 'Stage_Number', 'Team'])\n",
    "# Replace all remaining zero values with NaN\n",
    "df = df.replace(0, np.nan)\n",
    "\n",
    "print(df.isna().sum())\n",
    "print(\"Amount of rows in df after cleanup: \", df[df.columns[0]].count())\n",
    "print(\"\\ndone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PI 7: Part 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Appropriate Machine Learning Models\n",
    "In this section, the selection of appropriate machine learning models in the context of the current dataset will be discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Categorical and Regression Trees\n",
    "For the first model, the use of Categorical and Regression Trees will be discussed. As stated in *part 2* of the assignment, CART is a decision tree, that classifies records based on the conditions in the *decision nodes*, where the final classification, or regression, is determined in the *leaf nodes*. See the code from <code>pi7-2.ipynb</code> for a more in-depth explanation of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Argumentation\n",
    "A regression tree will be implemented due to the following reasons:\n",
    "- Handling possible non-linear relationships.\n",
    "- It does not require any normalization/standardization, making outcomes more interpretable.\n",
    "- It can handle outliers and missing values well.\n",
    "\n",
    "Arguments against implementing regression trees:\n",
    "- The more complex a tree becomes, the more prone it becomes to overfitting. There are methods for handling scenarios where overfitting can become an issue, like *pruning*.\n",
    "- Sensitive to hyperparameters.\n",
    "- Unsuitable for datasets with a large number of classes (see the arguments above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Implementation\n",
    "Below is am implementation of a regression tree on the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45966 entries, 659ed585810c65fe22255a5e4a9b7838 to 0292146b9196ec7a98903cb50dae48cd\n",
      "Data columns (total 26 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Rnk              45966 non-null  object \n",
      " 1   GC               40366 non-null  float64\n",
      " 2   BiB              45966 non-null  object \n",
      " 3   Rider            45966 non-null  object \n",
      " 4   Age              45966 non-null  int64  \n",
      " 5   UCI              1926 non-null   float64\n",
      " 6   Pnt              6859 non-null   float64\n",
      " 7   Length           40482 non-null  float64\n",
      " 8   Month            45966 non-null  int32  \n",
      " 9   Year             45966 non-null  int32  \n",
      " 10  Time_seconds     35316 non-null  float64\n",
      " 11  Timelag_seconds  39709 non-null  float64\n",
      " 12  Stage_Type_bin   4834 non-null   float64\n",
      " 13  fullname         45966 non-null  object \n",
      " 14  team             45966 non-null  object \n",
      " 15  country          45966 non-null  object \n",
      " 16  height           45914 non-null  float64\n",
      " 17  weight           45908 non-null  float64\n",
      " 18  PCS_Rnk          34332 non-null  float64\n",
      " 19  UCI_Rnk          33370 non-null  float64\n",
      " 20  AllTime_Rnk      26874 non-null  float64\n",
      " 21  Day_Pnt          45958 non-null  float64\n",
      " 22  GC_Pnt           45966 non-null  float64\n",
      " 23  TT_Pnt           45543 non-null  float64\n",
      " 24  Sprint_Pnt       45832 non-null  float64\n",
      " 25  Climb_Pnt        45809 non-null  float64\n",
      "dtypes: float64(17), int32(2), int64(1), object(6)\n",
      "memory usage: 9.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtc = df.drop(columns = ['UCI', 'Pnt', 'Stage_Type_bin'])\n",
    "df_dtc = df_dtc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12187 entries, a73d590113699f02caf57566c20a2ae7 to 4b35e4a129ae080c4ccec66ea79e0be3\n",
      "Data columns (total 23 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Rnk              12187 non-null  object \n",
      " 1   GC               12187 non-null  float64\n",
      " 2   BiB              12187 non-null  object \n",
      " 3   Rider            12187 non-null  object \n",
      " 4   Age              12187 non-null  int64  \n",
      " 5   Length           12187 non-null  float64\n",
      " 6   Month            12187 non-null  int32  \n",
      " 7   Year             12187 non-null  int32  \n",
      " 8   Time_seconds     12187 non-null  float64\n",
      " 9   Timelag_seconds  12187 non-null  float64\n",
      " 10  fullname         12187 non-null  object \n",
      " 11  team             12187 non-null  object \n",
      " 12  country          12187 non-null  object \n",
      " 13  height           12187 non-null  float64\n",
      " 14  weight           12187 non-null  float64\n",
      " 15  PCS_Rnk          12187 non-null  float64\n",
      " 16  UCI_Rnk          12187 non-null  float64\n",
      " 17  AllTime_Rnk      12187 non-null  float64\n",
      " 18  Day_Pnt          12187 non-null  float64\n",
      " 19  GC_Pnt           12187 non-null  float64\n",
      " 20  TT_Pnt           12187 non-null  float64\n",
      " 21  Sprint_Pnt       12187 non-null  float64\n",
      " 22  Climb_Pnt        12187 non-null  float64\n",
      "dtypes: float64(14), int32(2), int64(1), object(6)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_dtc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dtc.iloc[:, 0:22] # adjust row val\n",
    "y = df_dtc.iloc[:, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'monotonic_cst': None,\n",
       " 'random_state': None,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rider_id\n",
      "a73d590113699f02caf57566c20a2ae7         MORENO_Javier\n",
      "9be030c6ab77e67c18c3ddc46be4036c    ROJAS_José_Joaquín\n",
      "f0e3e679300f9caf7c477c1d3614a33b         MEYER_Cameron\n",
      "4485ac2bf252ae4395f1b850fa9e2c76        DURBRIDGE_Luke\n",
      "8b2ba2d2ad59c160774fc929f7f8a635        TRENTIN_Matteo\n",
      "                                           ...        \n",
      "8e8a5de5574e54c9a8ae5436158244ac           HAAS_Nathan\n",
      "f91fee92ec50e1574bd4c5fe3e2e18d2          DENNIS_Rohan\n",
      "b2a6130da47efd8e9946728441f7f16f          BAUHAUS_Phil\n",
      "5dbb77082695488e013574823562ec57       HIVERT_Jonathan\n",
      "4b35e4a129ae080c4ccec66ea79e0be3         BODNAR_Maciej\n",
      "Name: fullname, Length: 12187, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_dtc['fullname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'BURGHARDT Marcus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29729/3813196592.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/School/DatSci_PI7-1/.venv/lib/python3.13/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1386\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/School/DatSci_PI7-1/.venv/lib/python3.13/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \"\"\"\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         super()._fit(\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School/DatSci_PI7-1/.venv/lib/python3.13/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    248\u001b[0m             check_X_params = dict(\n\u001b[1;32m    249\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             X, y = validate_data(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School/DatSci_PI7-1/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2952\u001b[0m             \u001b[0;31m# :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2953\u001b[0m             \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"estimator\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2955\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2956\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2957\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"estimator\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m                 \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School/DatSci_PI7-1/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1052\u001b[0m                         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m                 raise ValueError(\n\u001b[1;32m   1058\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m                 \u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School/DatSci_PI7-1/.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School/DatSci_PI7-1/.venv/lib/python3.13/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     def __array__(\n\u001b[1;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'BURGHARDT Marcus'"
     ]
    }
   ],
   "source": [
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Boosted Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Metrics for Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Ideal Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bibliography"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
