{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "conn = lite.connect('cycling_big.db')\n",
    "\n",
    "riders_df = pd.read_sql_query('SELECT * FROM riders;', conn)\n",
    "races_df = pd.read_sql_query('SELECT * FROM race_results', conn)\n",
    "\n",
    "print(\"Amount of rows in races_df: \", races_df[races_df.columns[0]].count())\n",
    "print(\"Amount of rows in riders_df: \", riders_df[riders_df.columns[0]].count())\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\"\"\"\n",
    "Convert the 'Date' column to DateTime format\n",
    "Regarding races_df dataframe!\n",
    "\"\"\"\n",
    "\n",
    "races_df['Date'] = pd.to_datetime(races_df['Date'], errors='coerce', format='%d %B %Y')\n",
    "\n",
    "# Remove rows where 'Date' is NaT\n",
    "races_df = races_df.dropna(subset=['Date'])\n",
    "\n",
    "# Normalize 'Date' to strip out time if it's present (this keeps just the date part)\n",
    "races_df['Date'] = races_df['Date'].dt.normalize()\n",
    "\n",
    "# Extract the month and year from the column and put them in their own columns\n",
    "races_df['Month'] = races_df['Date'].dt.month\n",
    "races_df['Year'] = races_df['Date'].dt.year\n",
    "\n",
    "print(\"Unique values for 'months': \", races_df['Month'].unique(), \"\\n\")\n",
    "print(\"Unique values for 'years': \",races_df['Year'].unique())\n",
    "\n",
    "\"\"\"\n",
    "Converting the timetable to total seconds\n",
    "\"\"\"\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    # Remove commas and any spaces\n",
    "    time_str = time_str.replace(',', '').strip()\n",
    "\n",
    "    # Check the consistency of the time format using regular expressions\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2}):(\\d{2})', time_str)\n",
    "    if match:\n",
    "        hours, minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "        if total_seconds == 0:\n",
    "            return np.nan\n",
    "        return total_seconds\n",
    "\n",
    "    # Do the same as the loop above, but now for MM:SS format.\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2})', time_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        if total_seconds == 0:\n",
    "            return np.nan  \n",
    "        return total_seconds\n",
    "\n",
    "    # Do the same but for 0:00, 0:01, etc.\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{1,2})', time_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        if total_seconds == 0:\n",
    "            return np.nan \n",
    "        return total_seconds\n",
    "    \n",
    "    # If format doesn't match, return NaN\n",
    "    return np.nan\n",
    "\n",
    "# Apply the conversion function to the 'Time' column\n",
    "races_df['Time_seconds'] = races_df['Time'].apply(time_to_seconds)\n",
    "\n",
    "# print(races_df[['Time', 'Time_seconds']].head(10))\n",
    "# races_df.info()\n",
    "\n",
    "\"\"\"\n",
    "Converting timelag to total seconds\n",
    "\"\"\"\n",
    "\n",
    "def timelag_to_seconds(timelag_str):\n",
    "    # Check if the timelag_str contains missing values\n",
    "    if pd.isna(timelag_str):\n",
    "        return np.nan\n",
    "\n",
    "    timelag_str = timelag_str.lstrip('+').strip()\n",
    "\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2}):(\\d{2})', timelag_str)\n",
    "    if match:\n",
    "        hours, minutes, seconds = map(int, match.groups())\n",
    "        return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "    match = re.match(r'(\\d{1,2}):(\\d{2})', timelag_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        return minutes * 60 + seconds\n",
    "    \n",
    "    match = re.match(r'(\\d+):(\\d+)', timelag_str)\n",
    "    if match:\n",
    "        minutes, seconds = map(int, match.groups())\n",
    "        return minutes * 60 + seconds\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "races_df['Timelag_seconds'] = races_df['Timelag'].apply(timelag_to_seconds)\n",
    "\n",
    "races_df['Timelag_seconds'] = races_df['Timelag_seconds'].replace(0.0, np.nan)\n",
    "\n",
    "print(races_df[['Timelag', 'Timelag_seconds']].head(10))\n",
    "\n",
    "\"\"\"\n",
    "Converting distance into single numerical value\n",
    "This means stripping 'km' from string and converting the remaining values into float64\n",
    "\"\"\"\n",
    "\n",
    "races_df['Length'] = races_df['Length'].str.replace(' km', '', regex=False)\n",
    "\n",
    "# Convert to numeric and replace 0 with NaN\n",
    "races_df['Length'] = pd.to_numeric(races_df['Length'], errors='coerce')  # Convert to numeric and handle errors\n",
    "\n",
    "# Replace 0 values with NaN\n",
    "races_df['Length'] = races_df['Length'].replace(0.0, np.nan)\n",
    "\n",
    "# print(races_df['Length'])\n",
    "\n",
    "\"\"\"\n",
    "Splitting values from 'rdr' and putting the split values into separate columns\n",
    "\"\"\"\n",
    "\n",
    "# Function to convert the 'rdr' string to separate ranking columns\n",
    "def extract_rankings(rdr_str):\n",
    "    try:\n",
    "        # Converting str to dict\n",
    "        rankings = ast.literal_eval(rdr_str)\n",
    "        \n",
    "        # Extracting ranks, while also handling missing keys\n",
    "        pcs_rnk = rankings.get('PCS Ranking', np.nan)\n",
    "        uci_rnk = rankings.get('UCI World Ranking', np.nan)\n",
    "        alltime_rnk = rankings.get('Specials | All Time Ranking', np.nan)\n",
    "        \n",
    "        return pd.Series([pcs_rnk, uci_rnk, alltime_rnk])\n",
    "    except:\n",
    "        return pd.Series([np.nan, np.nan, np.nan])\n",
    "\n",
    "# Apply the function to the 'rdr' column\n",
    "riders_df[['PCS_Rnk', 'UCI_Rnk', 'AllTime_Rnk']] = riders_df['rdr'].apply(extract_rankings)\n",
    "\n",
    "# Entries should be turned into numeric values, where errors get turned into NaN\n",
    "riders_df['PCS_Rnk'] = pd.to_numeric(riders_df['PCS_Rnk'], errors='coerce')\n",
    "riders_df['UCI_Rnk'] = pd.to_numeric(riders_df['UCI_Rnk'], errors='coerce')\n",
    "riders_df['AllTime_Rnk'] = pd.to_numeric(riders_df['AllTime_Rnk'], errors='coerce')\n",
    "\n",
    "# print(riders_df[['fullname', 'PCS_Rnk', 'UCI_Rnk', 'AllTime_Rnk']])\n",
    "\n",
    "\"\"\"\n",
    "Convert stage types to binary with label encoding\n",
    "\"\"\"\n",
    "\n",
    "races_df['Stage_Type_bin'] = races_df['Stage_Type'].map({'RR': 0, 'ITT': 1})\n",
    "\n",
    "\"\"\"\n",
    "Splitting values from 'pps' and putting those values into separate columns.\n",
    "\"\"\"\n",
    "\n",
    "def extract_points(pps_str):\n",
    "    try:\n",
    "        points = ast.literal_eval(pps_str)\n",
    "        \n",
    "        day_pnt = np.nan if points.get('One day races', '0') == '0' else points.get('One day races', np.nan)\n",
    "        gc_pnt = np.nan if points.get('GC', '0') == '0' else points.get('GC', np.nan)\n",
    "        tt_pnt = np.nan if points.get('Time trial', '0') == '0' else points.get('Time trial', np.nan)\n",
    "        sprint_pnt = np.nan if points.get('Sprint', '0') == '0' else points.get('Sprint', np.nan)\n",
    "        climb_pnt = np.nan if points.get('Climber', '0') == '0' else points.get('Climber', np.nan)\n",
    "        \n",
    "        return pd.Series([day_pnt, gc_pnt, tt_pnt, sprint_pnt, climb_pnt])\n",
    "    except:\n",
    "        return pd.Series([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "riders_df[['Day_Pnt', 'GC_Pnt', 'TT_Pnt', 'Sprint_Pnt', 'Climb_Pnt']] = riders_df['pps'].apply(extract_points)\n",
    "\n",
    "riders_df['Day_Pnt'] = pd.to_numeric(riders_df['Day_Pnt'], errors='coerce')\n",
    "riders_df['GC_Pnt'] = pd.to_numeric(riders_df['GC_Pnt'], errors='coerce')\n",
    "riders_df['TT_Pnt'] = pd.to_numeric(riders_df['TT_Pnt'], errors='coerce')\n",
    "riders_df['Sprint_Pnt'] = pd.to_numeric(riders_df['Sprint_Pnt'], errors='coerce')\n",
    "riders_df['Climb_Pnt'] = pd.to_numeric(riders_df['Climb_Pnt'], errors='coerce')\n",
    "\n",
    "# print(riders_df[['fullname', 'Day_Pnt', 'GC_Pnt', 'TT_Pnt', 'Sprint_Pnt', 'Climb_Pnt']])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Join the rider and race tables together, using the rider_id as an index\n",
    "\"\"\"\n",
    "print(\"Amount of rows in races_df post cleanup: \", races_df[races_df.columns[0]].count())\n",
    "print(\"Amount of rows in riders_df post cleanup: \", riders_df[riders_df.columns[0]].count())\n",
    "\n",
    "df = races_df.set_index('rider_id').join(riders_df.set_index('rider_id'), how = 'inner')\n",
    "\n",
    "print(\"Amount of rows in df: \", df[df.columns[0]].count())\n",
    "\n",
    "\"\"\"\n",
    "Dropping columns that are not needed for analysis\n",
    "\"\"\"\n",
    "\n",
    "# Note: fix the long list\n",
    "df.drop(['Time', 'Timelag', 'rdr', 'pps', 'birthdate', 'rider_url', 'Race_url', 'Stage_url', 'Circuit', 'Race_Name', 'Stage_Name', 'Start', 'Finish', 'Category', 'Stage_Type'], axis=1, inplace=True)\n",
    "\n",
    "# Replace all remaining zero values with NaN\n",
    "df = df.replace(0, np.nan)\n",
    "\n",
    "print(df.isna().sum())\n",
    "print(\"Amount of rows in df after cleanup: \", df[df.columns[0]].count())\n",
    "print(\"\\ndone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PI 7: Part 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Appropriate Machine Learning Models\n",
    "In this section, the selection of appropriate machine learning models in the context of the current dataset will be discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Categorical and Regression Trees\n",
    "For the first model, the use of Categorical and Regression Trees will be discussed. As stated in *part 2* of the assignment, CART is a decision tree, that classifies records based on the conditions in the *decision nodes*, where the final classification, or regression, is determined in the *leaf nodes*. See the code from <code>pi7-2.ipynb</code> for a more in-depth explanation of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Argumentation\n",
    "A regression tree will be implemented due to the following reasons:\n",
    "- Handling possible non-linear relationships.\n",
    "- It does not require any normalization/standardization, making outcomes more interpretable.\n",
    "- It can handle outliers and missing values well.\n",
    "\n",
    "Arguments against implementing regression trees:\n",
    "- The more complex a tree becomes, the more prone it becomes to overfitting. There are methods for handling scenarios where overfitting can become an issue, like *pruning*.\n",
    "- Sensitive to hyperparameters.\n",
    "- Unsuitable for datasets with a large number of classes (see the arguments above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Implementation\n",
    "Below is am implementation of a regression tree on the current dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Boosted Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Metrics for Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Ideal Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bibliography"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
